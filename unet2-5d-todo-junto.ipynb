{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport tifffile\nimport time\nimport tensorflow.keras.backend as K\n\n\nhue_path = '/kaggle/input/sperm25d/arriba_hue/'\nfor dirname, _, filenames in os.walk(hue_path):\n    print(dirname)\n\n    \nvalue_path = '/kaggle/input/sperm25d/arriba_value/'\nfor dirname, _, filenames in os.walk(hue_path):\n    print(dirname)\n\ngt_path = '/kaggle/input/sperm25d/ground_truth/ground_truth/'\ngt_folders = sorted(os.listdir(gt_path))\ngt_folders.pop(2)\nprint(gt_folders[0])\n\ndef listar_paths(num_exp,exp):\n    gt_path_exp = gt_path + exp\n    value_tif = value_path + exp + \"_arriba_value.tif\"\n\n    # Process the TIFF stacks as needed\n    files_coord_flagelo = sorted(os.listdir(gt_path_exp))\n    \n    mask_files = []\n    for i in range(len(files_coord_flagelo)):\n        mask_files.append(os.path.join(gt_path_exp, files_coord_flagelo[i]))\n    \n    pos=0\n    if (num_exp==0 or num_exp==1 or num_exp==8 or num_exp==10):\n        pos = np.arange( len(files_coord_flagelo))\n    else:\n        pos = np.arange(1, len(files_coord_flagelo) + 1)\n    vt = [value_tif] * len(pos)\n    \n    return list(zip(vt, pos, mask_files))\n\ntrain_files = []\nvalid_files = []\ntest_files = []\n\nfor i in range(len(gt_folders)):\n    \n    paths = listar_paths(i,gt_folders[i])\n    \n    if i < 5:\n        train_files += paths\n    elif i < 7:\n        valid_files += paths\n    else:\n        test_files += paths\n        \nprint(train_files[0])\n\n\n\ndef preprocess_input(input_image_path, key, mask_image_path):\n    # Load and preprocess input image\n    input_image = tifffile.imread(input_image_path, key=key) \n    \n    # Load and preprocess output mask image\n    coords = pd.read_csv(mask_image_path)\n    mat = np.zeros((480, 640))\n    for i in range(len(coords)):\n        x=coords.iloc[i].x\n        y=coords.iloc[i].y\n        if y<mat.shape[0] and x<mat.shape[0]:\n            mat[y-1:y+2, x-1:x+2] = 1\n    \n    return input_image, mat\n\ndef data_generator(file_pairs, batch_size):\n    num_samples = len(file_pairs)\n    while True:\n        # Shuffle the indices at the start of each epoch\n        indices = np.random.permutation(num_samples)\n        \n        for i in range(0, num_samples, batch_size):\n            batch_indices = indices[i:i+batch_size]\n            batch_file_pairs = [file_pairs[j] for j in batch_indices]\n            \n            batch_inputs = []\n            batch_masks = []\n            \n            for input_path, key, mask_path in batch_file_pairs:\n                input_image, mask_image = preprocess_input(input_path, key, mask_path)\n                batch_inputs.append(input_image)\n                batch_masks.append(mask_image)\n            \n            batch_inputs = np.array(batch_inputs)\n            batch_masks = np.array(batch_masks,dtype=int)\n            \n            yield batch_inputs, batch_masks\n\n\nbatch_size = 32\ntrain_generator = data_generator(train_files, batch_size)\nvalidation_generator = data_generator(valid_files, batch_size)\n\ndef double_conv_block(x, n_filters):\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n    return x\n\ndef downsample_block(x, n_filters):\n    f = double_conv_block(x, n_filters)\n    p = layers.MaxPool2D(2)(f)\n    p = layers.Dropout(0.3)(p)\n    return f, p\n\ndef upsample_block(x, conv_features, n_filters):\n    # upsample\n    x = layers.Conv2DTranspose(n_filters, 3, strides=2, padding=\"same\")(x)\n    # concatenate\n    x = layers.concatenate([x, conv_features])\n    # dropout\n    x = layers.Dropout(0.3)(x)\n    # Conv2D twice with ReLU activation\n    x = double_conv_block(x, n_filters)\n    return x\n\ndef build_unet_model():\n    # inputs\n    inputs = layers.Input(shape=(480, 640, 1))\n\n    # encoder: contracting path - downsample\n    # 1 - downsample\n    n_filters_first = 16\n    f1, p1 = downsample_block(inputs, n_filters_first)\n    # 2 - downsample\n    f2, p2 = downsample_block(p1, n_filters_first * 2)\n    # 3 - downsample\n    f3, p3 = downsample_block(p2, n_filters_first * 4)\n    # 3 - downsample\n    #f4, p4 = downsample_block(p3, n_filters_first * 8)\n\n\n    # 5 - bottleneck\n    bottleneck = double_conv_block(p3, n_filters_first * 8)\n\n    # decoder: expanding path - upsample\n    #u6 = upsample_block(bottleneck, f4, n_filters_first * 8)\n    \n    # 7 - upsample\n    u7 = upsample_block(bottleneck, f3, n_filters_first * 4)\n    # 8 - upsample\n    u8 = upsample_block(u7, f2, n_filters_first * 2)\n    # 9 - upsample\n    u9 = upsample_block(u8, f1, n_filters_first)\n\n    # outputs\n    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(u9)\n\n    # U-Net model with TensorFlow's Functional API\n    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n\n    return unet_model\n\nunet_model = build_unet_model()\n\ndef weighted_bce(ground_truth,y_pred):\n    ground_truth = tf.cast(ground_truth,float)\n    weights =  ground_truth * 75 + 1.\n    bce = K.binary_crossentropy(ground_truth, y_pred)\n    weighted_bce = K.mean(bce * weights)\n    return weighted_bce\n\ncheckpoint_filepath = 'unet2-5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_loss',  # Change to 'val_loss' to monitor weighted BCE loss\n    mode='min',  # Change to 'min' since we are monitoring loss\n    save_best_only=True\n)\n\n\n\nm = tf.keras.metrics.MeanIoU(num_classes=2)\n\ndef dice_loss(y_true, y_pred, smooth=1e-7):\n    intersection = K.sum(y_true * y_pred)\n    union = K.sum(y_true) + K.sum(y_pred)\n    dice = (2.0 * intersection + smooth) / (union + smooth)\n    dice_loss = 1.0 - dice\n    return dice_loss\n\ndef dice_coefficient(y_true, y_pred, smooth=1e-7):\n    intersection = K.sum(y_true * y_pred)\n    union = K.sum(y_true) + K.sum(y_pred)\n    dice = (2.0 * intersection + smooth) / (union + smooth)\n    return dice\n\n# Add the Dice coefficient metric to your model compilation\nunet_model.compile(tf.keras.optimizers.Adam(learning_rate=0.004),\n              loss=weighted_bce,#tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy',dice_coefficient])\n\nunet_model.summary()\n\n# Start the timer\n\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n\n# Define the EarlyStopping callback\nearly_stopping_callback = EarlyStopping(\n    monitor='val_loss',  # Monitor validation loss\n    patience=10,  # Number of epochs with no improvement after which training will be stopped\n    restore_best_weights=True  # Restore the weights of the best-performing model\n)\n\n\n# Define the ReduceLROnPlateau callback\nreduce_lr_callback = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,  # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n    patience=5,  # Number of epochs with no improvement after which learning rate will be reduced\n    verbose=1  # Print a message when the learning rate is reduced\n)\n\nstart_time = time.time()\n\n# Train the model\nunet_model.fit(\n    train_generator,\n    steps_per_epoch=int(np.floor(len(train_files) / batch_size)),\n    epochs=30,\n    validation_data=validation_generator,\n    validation_steps=int(np.floor(len(valid_files) / batch_size)),\n    callbacks=[model_checkpoint_callback,early_stopping_callback,reduce_lr_callback]\n)\n\n# Calculate the training time\ntraining_time = time.time() - start_time\n\n# Print the training time\nprint(\"Training time:\", training_time, \"seconds\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-05T22:09:59.066356Z","iopub.execute_input":"2023-06-05T22:09:59.066729Z","iopub.status.idle":"2023-06-05T22:16:52.797320Z","shell.execute_reply.started":"2023-06-05T22:09:59.066698Z","shell.execute_reply":"2023-06-05T22:16:52.772745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_model.fit(\n    train_generator,\n    steps_per_epoch=int(np.floor(len(train_files) / batch_size)),\n    epochs=30,\n    validation_data=validation_generator,\n    validation_steps=int(np.floor(len(valid_files) / batch_size)),\n    callbacks=[model_checkpoint_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T20:44:29.809884Z","iopub.execute_input":"2023-06-05T20:44:29.810846Z","iopub.status.idle":"2023-06-05T21:06:52.308267Z","shell.execute_reply.started":"2023-06-05T20:44:29.810809Z","shell.execute_reply":"2023-06-05T21:06:52.307186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the second training image\nsample_index = 1\ninput_path, _, mask_path = train_files[sample_index]\n\n# Preprocess the input image and ground truth mask\ninput_image, ground_truth = preprocess_input(input_path, 1, mask_path)\n\n# Expand dimensions for prediction\ninput_image = np.expand_dims(input_image, axis=0)\n\n# Generate prediction\nprediction = unet_model.predict(input_image)\n\n# Plot the images\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original Image\naxs[0].imshow(input_image[0], cmap=\"gray\")\naxs[0].set_title(\"Original Image\")\naxs[0].axis(\"off\")\n\n# Ground Truth\naxs[1].imshow(ground_truth, cmap=\"gray\")\naxs[1].set_title(\"Ground Truth\")\naxs[1].axis(\"off\")\n\n# Prediction\naxs[2].imshow(prediction[0], cmap=\"gray\")\naxs[2].set_title(\"Prediction\")\naxs[2].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T22:05:41.217530Z","iopub.execute_input":"2023-06-05T22:05:41.218111Z","iopub.status.idle":"2023-06-05T22:05:41.775666Z","shell.execute_reply.started":"2023-06-05T22:05:41.218078Z","shell.execute_reply":"2023-06-05T22:05:41.774762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the file path for saving the model\nmodel_path = 'unet2-5D.h5'\n\n# Save the model\nunet_model.save(model_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T22:05:38.819580Z","iopub.execute_input":"2023-06-05T22:05:38.822020Z","iopub.status.idle":"2023-06-05T22:05:38.941964Z","shell.execute_reply.started":"2023-06-05T22:05:38.821992Z","shell.execute_reply":"2023-06-05T22:05:38.940926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the directory path for saving the model\nmodel_dir = 'unet25D-tf/'\n\n# Save the model in SavedModel format\ntf.saved_model.save(unet_model, model_dir)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = unet_model.history\n\n# Retrieve the training loss and validation loss\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Retrieve the number of epochs\nepochs = range(1, len(train_loss) + 1)\n\n# Plot the training and validation loss\nplt.plot(epochs, train_loss, 'bo-', label='Training Loss')\nplt.plot(epochs, val_loss, 'go-', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T22:04:53.366829Z","iopub.execute_input":"2023-06-05T22:04:53.367518Z","iopub.status.idle":"2023-06-05T22:05:38.817701Z","shell.execute_reply.started":"2023-06-05T22:04:53.367485Z","shell.execute_reply":"2023-06-05T22:05:38.816799Z"},"trusted":true},"execution_count":null,"outputs":[]}]}